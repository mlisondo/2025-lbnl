{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec3808b",
   "metadata": {},
   "source": [
    "# Transformers Tutorial: Part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2285437c-72ac-42c7-a70e-655b8339ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import load_data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb33ea9e-e41d-4dc8-ae2e-340089f9924e",
   "metadata": {},
   "source": [
    "### Let's open the training and validation files containing examples for top quarks (signal) and QCD jets (background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89bbb048-dc2f-4f03-9f02-f3f4e53fb9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/global/cfs/cdirs/trn016/transformer'\n",
    "train_data = load_data('top',input_folder,batch=256,dataset_type='train',num_evt = 100_000)\n",
    "val_data = load_data('top',input_folder,batch=256,dataset_type='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aeec21c-cfe9-47e3-9250-e14724fddce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 390 batches of events for training and 1574 for validation\n"
     ]
    }
   ],
   "source": [
    "print (f\"Loading {len(train_data)} batches of events for training and {len(val_data)} for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38a93c-14f3-49ec-90c7-65baeb1a10c8",
   "metadata": {},
   "source": [
    "### We Now need to create a model that will take the data as input and predict a label for each data entry. Let's create a config file with the network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c62ed545-402a-4b3d-aa49-60c3926efd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'num_layers': 2,\n",
    "    'hidden_dim': 64,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704f5a1f-61d6-4882-a80a-008e61a8ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim, dim)\n",
    "        self.k = nn.Linear(dim, dim)\n",
    "        self.v = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        B, L, C = x.shape\n",
    "        \n",
    "        q = self.q(x)*mask\n",
    "        k = self.k(x)*mask\n",
    "        v = self.v(x)*mask\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) #Matrix multiplication: (B, L, C) x (B, C, L) = (B, L, L) shape\n",
    "        attn = attn.softmax(dim=-1) #Normalization\n",
    "        x = (attn @ v) # Matrix multiplication: (B, L, L) x (B, L, C) = (B, L, C)\n",
    "\n",
    "        return x*mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d2a3539-2431-4f5a-8910-f399b4920d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, config, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, config[\"hidden_dim\"])\n",
    "        \n",
    "        layers = []\n",
    "        for _ in range(config[\"num_layers\"]):\n",
    "            layers.append(Attention(config[\"hidden_dim\"]))\n",
    "        self.hidden_layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.output_layer = nn.Linear(config[\"hidden_dim\"], num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        zero_pad_mask = (inputs[:, :, 2] != 0).unsqueeze(-1).float()\n",
    "        x = self.input_layer(inputs) * zero_pad_mask\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x,zero_pad_mask)\n",
    "        x = x.mean(1)  # aggregate over particles\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a778d5c8-5f18-401b-b944-38422d1164ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleTransformer(input_dim=4,config=config) #remember the inputs are delta eta, delta phi, log(pT), log(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f95bf4-8f95-4742-aa4b-dc6f3e211511",
   "metadata": {},
   "source": [
    "### Now we are going to create the training class that will train the model, but first, let's set up the learning rate and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73397f76-9973-4bad-9bf7-146a01408397",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam\n",
    "lr = 5e-4\n",
    "epochs = 100\n",
    "patience = 10 # Number of consecutive epochs to stop the training if the validation loss does not improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e6c7534-3b5f-4d2d-b53c-3f17389a9940",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = utils.Trainer(train_data,val_data,model,lr,optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ea3830-5f46-45eb-b27c-0478c17451de",
   "metadata": {},
   "source": [
    "### Let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb53f3c-c01d-4d0c-a16f-28fb5704a609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss=0.5748, validation loss=0.4991\n"
     ]
    }
   ],
   "source": [
    "trainer.train(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc16252-f4ae-4027-88ef-23b258ee0ffd",
   "metadata": {},
   "source": [
    "### Now let's evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256fe45c-70dc-46b6-a9db-1a8661a8bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_data('top',input_folder,batch=128,dataset_type='test')\n",
    "predictions, labels = trainer.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6737db7-d095-4762-a2fd-2b38aec08642",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.print_metrics(predictions,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4565450-45ce-4b2e-b5ec-0fb605a44fc3",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"vertical-align: top; padding-right: 20px;\">\n",
    "\n",
    "### Wait, why is it just as good as the DeepSets model?\n",
    "A: Although attention is essential to capture correlations, the standard Transformer Architecture we use also combines additional ingredients such as linear transformations, normalization, and skip connections. These additional operations make the architecture more expressive and more stable.\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"transformer.webp\" alt=\"Transformer Block\" width=\"300\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b43c46-c082-4669-9669-df9cd769a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.att = Attention(dim)\n",
    "        self.proj1 = nn.Linear(dim, dim)\n",
    "        self.proj2 = nn.Linear(dim, dim)\n",
    "        self.activation = nn.GELU()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x + self.att(self.norm1(x),mask) # Attention on normalized inputs is added to the inputs\n",
    "        x = self.activation(self.proj1(x))*mask #Add a linear layer + non-linear activation        \n",
    "        x = x + self.proj2(self.norm2(x))*mask #Add another linear layer on normalized inputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789fff44-b449-442c-a607-b7dcd654935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, config, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, config[\"hidden_dim\"])\n",
    "        \n",
    "        layers = []\n",
    "        for _ in range(config[\"num_layers\"]):\n",
    "            layers.append(TransformerBlock(config[\"hidden_dim\"]))\n",
    "        self.hidden_layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.output_layer = nn.Linear(config[\"hidden_dim\"], num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        zero_pad_mask = (inputs[:, :, 2] != 0).unsqueeze(-1).float()\n",
    "        x = self.input_layer(inputs) * zero_pad_mask\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x,zero_pad_mask)\n",
    "        x = x.mean(1)  # aggregate over particles\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef8519-4b95-486a-9bbd-ec1baafcda8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(input_dim=4,config=config) \n",
    "optimizer = torch.optim.Adam\n",
    "lr = 5e-4\n",
    "epochs = 100\n",
    "patience = 10 # Number of consecutive epochs to stop the training if the validation loss does not improve\n",
    "trainer = utils.Trainer(train_data,val_data,model,lr,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fe0bae-7429-4398-ace5-db793e47c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3f5ed7-2f4b-478f-be48-ac20e8cb375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels = trainer.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c509579-16cb-4d7d-ab3b-aacaccc69082",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.print_metrics(predictions,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c1c41-0f8f-4fdf-897f-81f4ed0da18d",
   "metadata": {},
   "source": [
    "### Try changing the hyperparameters of the model to see if you can improve the results!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.6.0",
   "language": "python",
   "name": "pytorch-2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
